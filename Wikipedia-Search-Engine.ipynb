{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up environment for Apache Spark\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir(r\"C:\\spark\\spark-files\")\n",
    "os.curdir\n",
    "\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\spark'\n",
    "\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"pyspark.zip\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME,\"python\",\"lib\",\"py4j-0.10.4-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "import operator\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abraham', 'Lincoln', 'Gettysburg']\n"
     ]
    }
   ],
   "source": [
    "#Taking search string as input from user when integrated to web\n",
    "list = sys.argv\n",
    "st = str(list[1])\n",
    "wo = st.lower().split()\n",
    "\n",
    "#Temporarily taking manual input here, for testing\n",
    "st= \"Abraham Lincoln Gettysburg\"\n",
    "wo = st.split()\n",
    "print(wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setting up Spark Context, Run only once in a single session\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkTFIDF\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading and extracting data\n",
    "\n",
    "rawData = sc.textFile(\"wikipedia_data.tsv\") #Located at C:\\spark\\spark-files\n",
    "fields = rawData.map(lambda x: x.split(\"\\t\"))\n",
    "documents = fields.map(lambda x: x[3].split(\" \"))\n",
    "documentNames = fields.map(lambda x: x[1])\n",
    "\n",
    "#rawData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating TF-IDF\n",
    "\n",
    "hashingTF = HashingTF(100000)  \n",
    "article_hash_value = hashingTF.transform(documents)\n",
    "article_hash_value.cache() #storing value in-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[9] at mapPartitions at PythonMLLibAPI.scala:1335"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = IDF(minDocFreq=2).fit(article_hash_value)\n",
    "tfidf = idf.transform(article_hash_value)\n",
    "tfidf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_of_given_word(article_hash_value, word_hash):\n",
    "    gtbTF = hashingTF.transform([word_hash])\n",
    "    gtbHashValue1 = int(gtbTF.indices[0])\n",
    "    gtbRelevance1 = tfidf.map(lambda x: x[gtbHashValue1] )\n",
    "    y= [] \n",
    "    for z in gtbRelevance1.toLocalIterator():\n",
    "        y.append(z)    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(len(wo)):\n",
    "    c = []\n",
    "    c= df_of_given_word(article_hash_value,wo[i])\n",
    "    if i == 0:\n",
    "        b = c\n",
    "    else:\n",
    "        b = map(operator.add, b,c)\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best document for Abraham Lincoln Gettysburg is:\n",
      "(u'Abraham Lincoln', 683.64280658581845)\n"
     ]
    }
   ],
   "source": [
    "dn=[]\n",
    "for z in documentNames.toLocalIterator():\n",
    "#for z in documentNames.collect():\n",
    "        dn.append(z)\n",
    "zipp = map(lambda x,y: (x,y),dn,b) \n",
    "# print(list(zipp))\n",
    "zipp.sort(key=lambda tup: tup[1], reverse = True)\n",
    "zipp.sort(key=lambda tup: tup[1], reverse = True)\n",
    "print(\"Best document for \" +st+  \" is:\")\n",
    "print(zipp[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
